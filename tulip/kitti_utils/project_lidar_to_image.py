#!/usr/bin/env python3
"""
Script to project lidar points onto camera image using info files generated by gen_info.py.

This script:
1. Loads the pickle file generated by gen_info.py
2. Takes the first sample [0]
3. Loads lidar point cloud from .bin file
4. Loads camera image
5. Projects lidar points onto the image using calibration data
6. Visualizes the result
"""

import argparse
import pickle
import numpy as np
import cv2
import os
import sys
from sample_kitti_dataset import load_from_bin

# Add parent directory to path to import from tulip
script_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(script_dir)
sys.path.insert(0, parent_dir)

from tulip.util.datasets import RVWithImageDataset, transforms, ScaleTensor, FilterInvalidPixels, LogTransform, DownsampleTensor

# Try to import matplotlib, but make it optional
try:
    import matplotlib.pyplot as plt
    MATPLOTLIB_AVAILABLE = True
except ImportError:
    MATPLOTLIB_AVAILABLE = False
    print("Warning: matplotlib not available, will use cv2 for visualization")

# Try to import open3d for 3D visualization
try:
    import open3d as o3d
    OPEN3D_AVAILABLE = True
except ImportError:
    OPEN3D_AVAILABLE = False
    print("Warning: open3d not available, 3D visualization will be skipped")




def project_points_to_camera(points, lidar2img_matrices, img_shapes):
    """
    Project 3D points to camera image coordinates using einsum for efficient broadcasting.
    Matches the implementation from nuScenes demo_lidar_camera_projection.py.
    
    Args:
        points: (B, N, 3) or (N, 3) array of 3D points in lidar frame
        lidar2img_matrices: (B, num_cam, 4, 4) or (num_cam, 4, 4) stacked transformation matrices
        img_shapes: (B, num_cam, 2) or (num_cam, 2) array of (H, W) image shapes
        
    Returns:
        projected_points: (B, num_cam, N, 2) or (num_cam, N, 2) array of image coordinates
        valid_masks: (B, num_cam, N) or (num_cam, N) boolean masks for points within image bounds
    """
    # Handle both batched and non-batched inputs
    if points.ndim == 2:
        # Non-batched: add batch dimension
        points = points[None, ...]  # (1, N, 3)
        lidar2img_matrices = lidar2img_matrices[None, ...]  # (1, num_cam, 4, 4)
        img_shapes = img_shapes[None, ...]  # (1, num_cam, 2)
        squeeze_output = True
    else:
        squeeze_output = False
    
    imgH, imgW = img_shapes[0,0,0], img_shapes[0,0,1]
    
    B, N, _ = points.shape
    _, num_cam, _, _ = lidar2img_matrices.shape
    
    if N == 0:
        return (np.array([]).reshape(B, num_cam, 0, 2) if not squeeze_output 
                else np.array([]).reshape(num_cam, 0, 2)), \
               (np.array([]).reshape(B, num_cam, 0) if not squeeze_output 
                else np.array([]).reshape(num_cam, 0))
    
    # Convert to homogeneous coordinates
    points_homo = np.concatenate([points, np.ones((B, N, 1))], axis=-1)  # (B, N, 4)
    
    # Project to image coordinates using einsum for efficient broadcasting across batch and cameras
    # einsum('bcij,bnj->bcni', lidar2img_matrices, points_homo)
    # b: batch, c: camera, i: output dim, j: input dim, n: points
    reference_points_cam = np.einsum('bcij,bnj->bcni', lidar2img_matrices, points_homo)  # (B, num_cam, N, 4)
    
    # Convert from homogeneous coordinates
    eps = 1e-5

    mask = (reference_points_cam[..., 2:3] > eps)
    reference_points_cam = reference_points_cam[..., 0:2] / np.maximum(
        reference_points_cam[..., 2:3], np.ones_like(reference_points_cam[..., 2:3]) * eps)

    # Check bounds
    mask = mask & (reference_points_cam[..., 1:2] > 0.0) \
            & (reference_points_cam[..., 1:2] < imgH) \
            & (reference_points_cam[..., 0:1] < imgW) \
            & (reference_points_cam[..., 0:1] > 0.0)
    
    # Squeeze the last dimension from mask (it was kept from [..., 2:3] slicing)
    mask = mask.squeeze(-1)  # (B, num_cam, N)
                
    if squeeze_output:
        reference_points_cam = reference_points_cam[0]  # (num_cam, N, 2)
        mask = mask[0]  # (num_cam, N)
    
    return reference_points_cam, mask


def visualize_projection_cv2(image, points_img, valid_mask, points_lidar, output_path=None):
    """
    Visualize lidar points projected onto the image using cv2.
    
    Args:
        image: (H, W, 3) image array in RGB format
        points_img: (N, 2) image coordinates
        valid_mask: (N,) boolean mask for valid points
        points_lidar: (N, 3) original lidar points
        output_path: Optional path to save the visualization
    """
    # Convert RGB to BGR for cv2
    img_bgr = cv2.cvtColor(image.copy(), cv2.COLOR_RGB2BGR)
    
    # Get valid points
    valid_points_img = points_img[valid_mask].astype(np.int32)
    valid_points_lidar = points_lidar[valid_mask]
    
    if len(valid_points_img) > 0:
        # Color points by depth (distance from lidar)
        depths = np.linalg.norm(valid_points_lidar, axis=1)
        depth_normalized = np.clip((depths / 80.0) * 255, 0, 255).astype(np.uint8)
        
        # Create colormap (jet-like: blue -> green -> red)
        depth_colormap = cv2.applyColorMap(depth_normalized, cv2.COLORMAP_JET)
        
        # Draw points
        for i, (pt, color) in enumerate(zip(valid_points_img, depth_colormap)):
            cv2.circle(img_bgr, tuple(pt), 1, color[0].tolist(), -1)
    
    if output_path:
        cv2.imwrite(output_path, img_bgr)
        print(f"Saved visualization to {output_path}")
    else:
        # Display image
        cv2.imshow('Lidar Points Projected on Image', img_bgr)
        print("Press any key to close the window...")
        cv2.waitKey(0)
        cv2.destroyAllWindows()


def visualize_projection_matplotlib(image, points_img, valid_mask, points_lidar, output_path=None):
    """
    Visualize lidar points projected onto the image using matplotlib.
    
    Args:
        image: (H, W, 3) image array
        points_img: (N, 2) image coordinates
        valid_mask: (N,) boolean mask for valid points
        points_lidar: (N, 3) original lidar points
        output_path: Optional path to save the visualization
    """
    # Create figure with single plot
    fig, ax = plt.subplots(1, 1, figsize=(12, 8))
    
    ax.imshow(image)
    ax.set_title('Lidar Points Projected on Image', fontsize=14)
    ax.axis('off')
    
    # Get valid points
    valid_points_img = points_img[valid_mask]
    valid_points_lidar = points_lidar[valid_mask]
    
    if len(valid_points_img) > 0:
        # Color points by depth (distance from lidar)
        depths = np.linalg.norm(valid_points_lidar, axis=1)
        ax.scatter(
            valid_points_img[:, 0], 
            valid_points_img[:, 1],
            c=depths,
            s=1,
            cmap='jet',
            alpha=0.6,
            vmin=0,
            vmax=80  # Adjust based on your max range
        )
        # Note: colorbar removed as requested
    
    plt.tight_layout()
    
    if output_path:
        plt.savefig(output_path, dpi=150, bbox_inches='tight')
        print(f"Saved visualization to {output_path}")
    else:
        plt.show()


def visualize_3d_pointcloud(points_lidar, output_path=None):
    """
    Visualize 3D point cloud using Open3D.
    Uses offscreen rendering if display is not available.
    
    Args:
        points_lidar: (N, 3) or (N, 4) array of 3D points in lidar frame
        output_path: Optional path to save the visualization (screenshot)
    """
    if not OPEN3D_AVAILABLE:
        print("Warning: open3d not available, skipping 3D visualization")
        return
    
    # Extract x, y, z coordinates
    points_3d = points_lidar[:, :3].astype(np.float64)
    
    print(f"  Point cloud bounds:")
    print(f"    X: [{points_3d[:, 0].min():.2f}, {points_3d[:, 0].max():.2f}]")
    print(f"    Y: [{points_3d[:, 1].min():.2f}, {points_3d[:, 1].max():.2f}]")
    print(f"    Z: [{points_3d[:, 2].min():.2f}, {points_3d[:, 2].max():.2f}]")
    
    # Create Open3D point cloud
    pcd = o3d.geometry.PointCloud()
    pcd.points = o3d.utility.Vector3dVector(points_3d)
    
    # Color points by depth (distance from origin)
    depths = np.linalg.norm(points_3d, axis=1)
    
    # Normalize depths to [0, 1] for colormap
    depth_normalized = np.clip(depths / 80.0, 0, 1)
    
    # Apply jet colormap (using matplotlib if available, otherwise use simple gradient)
    if MATPLOTLIB_AVAILABLE:
        colors = plt.cm.jet(depth_normalized)[:, :3]  # Get RGB, ignore alpha
    else:
        # Simple blue-to-red gradient if matplotlib not available
        colors = np.zeros_like(points_3d)
        colors[:, 0] = depth_normalized  # Red increases with depth
        colors[:, 2] = 1.0 - depth_normalized  # Blue decreases with depth
    
    pcd.colors = o3d.utility.Vector3dVector(colors)
    
    # Compute center and bounds for better view
    center = points_3d.mean(axis=0)
    max_range = np.max(points_3d.max(axis=0) - points_3d.min(axis=0))
    
    print(f"  Point cloud center: {center}")
    print(f"  Max range: {max_range:.2f}")
    
    # Create visualization window (interactive)
    try:
        vis = o3d.visualization.Visualizer()
        vis.create_window(window_name="3D Point Cloud Visualization", width=1280, height=720)
        vis.add_geometry(pcd)
        
        # Set up coordinate frame (optional, shows axes) - size relative to point cloud
        coord_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=max_range * 0.1)
        vis.add_geometry(coord_frame)
        
        # Set view parameters - look at center of point cloud
        view_ctl = vis.get_view_control()
        # For KITTI: X forward, Y left, Z up
        # Camera should look from behind/above
        view_ctl.set_front([-0.5, -0.5, -0.7])  # Camera position
        view_ctl.set_lookat(center)  # Look at center of point cloud
        view_ctl.set_up([0, 0, 1])  # Z-up
        view_ctl.set_zoom(0.6)
        
        # Set render options
        render_opt = vis.get_render_option()
        render_opt.point_size = 1.0
        render_opt.background_color = np.asarray([0.1, 0.1, 0.1])  # Dark background
        
        if output_path:
            # Save screenshot
            base_path = output_path.rsplit('.', 1)[0] if '.' in output_path else output_path
            output_3d_path = f"{base_path}_3d.png"
            vis.capture_screen_image(output_3d_path)
            print(f"Saved 3D visualization screenshot to {output_3d_path}")
            vis.destroy_window()
        else:
            print("  Close the 3D visualization window to continue...")
            vis.run()
            vis.destroy_window()
            
    except Exception as e:
        print(f"  Failed to create visualization window: {e}")
        print("  Skipping 3D visualization. Make sure you have a display available or install Open3D with GUI support.")


def visualize_projection(image, points_img, valid_mask, points_lidar, output_path=None):
    """
    Visualize lidar points projected onto the image.
    Uses matplotlib if available, otherwise falls back to cv2.
    
    Args:
        image: (H, W, 3) image array
        points_img: (N, 2) image coordinates
        valid_mask: (N,) boolean mask for valid points
        points_lidar: (N, 3) original lidar points
        output_path: Optional path to save the visualization
    """
    if MATPLOTLIB_AVAILABLE:
        visualize_projection_matplotlib(image, points_img, valid_mask, points_lidar, output_path)
    else:
        visualize_projection_cv2(image, points_img, valid_mask, points_lidar, output_path)


def build_kitti_dataset(data_root: str, info_file: str, final_dim=None):
    """
    Build RVWithImageDataset for KITTI with minimal transforms (for projection only).
    
    Args:
        data_root: Root directory of KITTI data
        info_file: Info file name
        final_dim: Final image dimensions (H, W). If None, uses original image size from first sample.
    
    Returns:
        dataset: RVWithImageDataset instance
    """
    import mmcv
    
    # Create dummy transforms (we don't need actual transforms for projection)
    t_low_res = [transforms.ToTensor()]
    t_high_res = [transforms.ToTensor()]
    
    transform_low_res = transforms.Compose(t_low_res)
    transform_high_res = transforms.Compose(t_high_res)
    
    # Load info to get image dimensions if not specified
    if final_dim is None:
        info_path = os.path.join(data_root, info_file)
        infos = mmcv.load(info_path)
        if len(infos) > 0 and 'cam_infos' in infos[0] and 'image_02' in infos[0]['cam_infos']:
            cam_info = infos[0]['cam_infos']['image_02']
            final_dim = (cam_info['height'], cam_info['width'])  # (H, W)
        else:
            final_dim = (375, 1242)  # KITTI default image size (H, W)
    
    dataset = RVWithImageDataset(
        root=data_root,
        high_res_transform=transform_high_res,
        low_res_transform=transform_low_res,
        info_file=info_file,
        final_dim=final_dim,
    )
    
    # Override cam_names for KITTI (only image_02)
    dataset.cam_names = ['image_02']
    
    return dataset


def main():
    parser = argparse.ArgumentParser(
        description='Project lidar points onto camera image using RVWithImageDataset'
    )
    parser.add_argument(
        '--data_root',
        type=str,
        default='./data/kitti/',
        help='Root directory of KITTI data (default: ./data/kitti/)'
    )
    parser.add_argument(
        '--info_file',
        type=str,
        default='kitti_upsample_infos_train.pkl',
        help='Pickle file generated by gen_info.py (default: kitti_upsample_infos_train.pkl)'
    )
    parser.add_argument(
        '--sample_idx',
        type=int,
        default=100,
        help='Sample index to visualize (default: 0)'
    )
    parser.add_argument(
        '--output_path',
        type=str,
        default=None,
        help='Path to save the visualization (default: display interactively)'
    )
    args = parser.parse_args()
    
    data_root = args.data_root
    info_file = args.info_file
    sample_idx = args.sample_idx
    
    # Build dataset using RVWithImageDataset
    print(f"Building dataset from: {data_root}")
    print(f"Info file: {info_file}")
    dataset = build_kitti_dataset(data_root, info_file)
    print(f"Loaded {len(dataset)} samples")
    
    if sample_idx >= len(dataset):
        raise ValueError(f"Sample index {sample_idx} is out of range (max: {len(dataset)-1})")
    
    # Use dataset's __getitem__ method directly
    print(f"\nLoading sample {sample_idx} from dataset...")
    sample_data = dataset[sample_idx]
    
    # Unpack dataset output
    (
        sweep_imgs,
        sweep_intrins,
        sweep_lidar2img_rts,
        sweep_lidar2cam_rts,
        low_res_rv,
        high_res_rv,
        lidar2ego_mat,
        mask_sample,
        img_shape,
        sample_token,
    ) = sample_data
    
    print(f"  Sample token: {sample_token}")
    
    # Get sample info for accessing file paths
    sample_info = dataset.infos[sample_idx]
    print(f"  Scene token: {sample_info['scene_token']}")
    
    # Extract lidar2img matrix for image_02 (first and only camera)
    # sweep_lidar2img_rts has shape (num_cam, 4, 4) after torch.cat
    lidar2img_matrix = sweep_lidar2img_rts[0].numpy()  # (4, 4) for image_02
    print(f"\nLidar2Img matrix shape: {lidar2img_matrix.shape}")
    
    # Get image shape from dataset (may be resized)
    img_h, img_w = img_shape.numpy()
    print(f"  Dataset image shape: {img_h}x{img_w}")
    
    # Load lidar point cloud directly from .bin file (not from range view)
    lidar_info = sample_info['lidar_info']
    lidar_file = os.path.join(data_root, lidar_info['filename'])
    print(f"  Loading lidar from: {lidar_file}")
    
    if not os.path.exists(lidar_file):
        raise FileNotFoundError(f"Lidar file not found: {lidar_file}")
    
    points_lidar = load_from_bin(lidar_file)
    print(f"  Loaded {len(points_lidar)} lidar points")
    
    # Get camera info for loading original image
    cam_infos = sample_info['cam_infos']
    if 'image_02' not in cam_infos:
        raise ValueError("Camera 'image_02' not found in sample")
    
    cam_info = cam_infos['image_02']
    print(f"  Camera: image_02")
    print(f"  Original image size: {cam_info['width']}x{cam_info['height']}")
    
    # Load original camera image (not normalized) for visualization
    img_file = os.path.join(data_root, cam_info['filename'])
    print(f"  Loading image from: {img_file}")
    
    if not os.path.exists(img_file):
        raise FileNotFoundError(f"Image file not found: {img_file}")
    
    image = cv2.imread(img_file)
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    print(f"  Image shape: {image.shape}")
    
    # Prepare inputs for projection (matching nuScenes format)
    # Pass as non-batched: function will add batch dimension automatically
    lidar2img_matrices = lidar2img_matrix[None, ...]  # (1, 4, 4) - num_cam=1
    img_shapes = np.array([[img_h, img_w]])  # (1, 2)
    
    # Project lidar points to image (using same function as nuScenes)
    points_3d = points_lidar[:, :3]  # Only use x, y, z coordinates
    
    # Debug: Check z-values distribution
    print(f"\nPoint cloud statistics:")
    print(f"  Z range: [{points_3d[:, 2].min():.2f}, {points_3d[:, 2].max():.2f}]")
    print(f"  X range: [{points_3d[:, 0].min():.2f}, {points_3d[:, 0].max():.2f}]")
    print(f"  Y range: [{points_3d[:, 1].min():.2f}, {points_3d[:, 1].max():.2f}]")
    print(f"  Points with z > 0: {np.sum(points_3d[:, 2] > 0)} / {len(points_3d)}")
    print(f"  Points with z < 0: {np.sum(points_3d[:, 2] < 0)} / {len(points_3d)}")
    
    points_img_batched, valid_mask_batched = project_points_to_camera(
        points_3d,
        lidar2img_matrices,
        img_shapes
    )
    
    # Extract results (function returns squeezed output: (num_cam, N, 2))
    points_img = points_img_batched[0]  # (N, 2) - first (and only) camera
    valid_mask = valid_mask_batched[0]  # (N,) - first (and only) camera
    
    # Ensure mask is 1D
    valid_mask = np.squeeze(valid_mask)
    points_img = np.squeeze(points_img)
    
    # Ensure points_img is 2D (N, 2)
    if points_img.ndim == 1:
        points_img = points_img.reshape(-1, 2)
    
    # Debug: Check projected coordinates distribution
    if np.sum(valid_mask) > 0:
        valid_points_img = points_img[valid_mask]
        print(f"\nProjected coordinates statistics:")
        print(f"  V (vertical) range: [{valid_points_img[:, 1].min():.1f}, {valid_points_img[:, 1].max():.1f}] / {img_h}")
        print(f"  U (horizontal) range: [{valid_points_img[:, 0].min():.1f}, {valid_points_img[:, 0].max():.1f}] / {img_w}")
        print(f"  Points in top half (v < {img_h/2}): {np.sum(valid_points_img[:, 1] < img_h/2)}")
        print(f"  Points in bottom half (v >= {img_h/2}): {np.sum(valid_points_img[:, 1] >= img_h/2)}")
    
    # Scale points back to original image coordinates if dataset resized the image
    if img_h != image.shape[0] or img_w != image.shape[1]:
        scale_h = image.shape[0] / img_h
        scale_w = image.shape[1] / img_w
        points_img[:, 0] *= scale_w
        points_img[:, 1] *= scale_h
        print(f"  Scaled image coordinates by ({scale_w:.3f}, {scale_h:.3f}) to match original image")
    
    num_valid = np.sum(valid_mask)
    print(f"\nProjection results:")
    print(f"  Valid projections: {num_valid} / {len(points_lidar)} ({100*num_valid/len(points_lidar):.1f}%)")
    
    if num_valid > 0:
        valid_points_img = points_img[valid_mask]
        print(f"  Image coordinates range: u=[{valid_points_img[:, 0].min():.1f}, {valid_points_img[:, 0].max():.1f}], "
              f"v=[{valid_points_img[:, 1].min():.1f}, {valid_points_img[:, 1].max():.1f}]")
    
    # Visualize 2D projection
    visualize_projection(
        image,
        points_img,
        valid_mask,
        points_lidar,
        output_path=args.output_path
    )
    
    # Visualize 3D point cloud
    print("\nVisualizing 3D point cloud...")
    visualize_3d_pointcloud(
        points_lidar,
        output_path=args.output_path
    )


if __name__ == '__main__':
    main()
